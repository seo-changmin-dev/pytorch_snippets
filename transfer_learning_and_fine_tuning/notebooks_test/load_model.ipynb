{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg16_net = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1, progress=True)\n",
    "print(vgg16_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-5.5373e-01,  1.4270e-01,  5.2896e-01],\n",
       "          [-5.8312e-01,  3.5655e-01,  7.6566e-01],\n",
       "          [-6.9022e-01, -4.8019e-02,  4.8409e-01]],\n",
       "\n",
       "         [[ 1.7548e-01,  9.8630e-03, -8.1413e-02],\n",
       "          [ 4.4089e-02, -7.0323e-02, -2.6035e-01],\n",
       "          [ 1.3239e-01, -1.7279e-01, -1.3226e-01]],\n",
       "\n",
       "         [[ 3.1303e-01, -1.6591e-01, -4.2752e-01],\n",
       "          [ 4.7519e-01, -8.2677e-02, -4.8700e-01],\n",
       "          [ 6.3203e-01,  1.9308e-02, -2.7753e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.3254e-01,  1.2666e-01,  1.8605e-01],\n",
       "          [-4.2805e-01, -2.4349e-01,  2.4628e-01],\n",
       "          [-2.5066e-01,  1.4177e-01, -5.4864e-03]],\n",
       "\n",
       "         [[-1.4076e-01, -2.1903e-01,  1.5041e-01],\n",
       "          [-8.4127e-01, -3.5176e-01,  5.6398e-01],\n",
       "          [-2.4194e-01,  5.1928e-01,  5.3915e-01]],\n",
       "\n",
       "         [[-3.1432e-01, -3.7048e-01, -1.3094e-01],\n",
       "          [-4.7144e-01, -1.5503e-01,  3.4589e-01],\n",
       "          [ 5.4384e-02,  5.8683e-01,  4.9580e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7715e-01,  5.2149e-01,  9.8740e-03],\n",
       "          [-2.7185e-01, -7.1709e-01,  3.1292e-01],\n",
       "          [-7.5753e-02, -2.2079e-01,  3.3455e-01]],\n",
       "\n",
       "         [[ 3.0924e-01,  6.7071e-01,  2.0546e-02],\n",
       "          [-4.6607e-01, -1.0697e+00,  3.3501e-01],\n",
       "          [-8.0284e-02, -3.0522e-01,  5.4460e-01]],\n",
       "\n",
       "         [[ 3.1572e-01,  4.2335e-01, -3.4976e-01],\n",
       "          [ 8.6354e-02, -4.6457e-01,  1.1803e-02],\n",
       "          [ 1.0483e-01, -1.4584e-01, -1.5765e-02]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 7.7599e-02,  1.2692e-01,  3.2305e-02],\n",
       "          [ 2.2131e-01,  2.4681e-01, -4.6637e-02],\n",
       "          [ 4.6407e-02,  2.8246e-02,  1.7528e-02]],\n",
       "\n",
       "         [[-1.8327e-01, -6.7425e-02, -7.2120e-03],\n",
       "          [-4.8855e-02,  7.0427e-03, -1.2883e-01],\n",
       "          [-6.4601e-02, -6.4566e-02,  4.4235e-02]],\n",
       "\n",
       "         [[-2.2547e-01, -1.1931e-01, -2.3425e-02],\n",
       "          [-9.9171e-02, -1.5143e-02,  9.5385e-04],\n",
       "          [-2.6137e-02,  1.3567e-03,  1.4282e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6520e-02, -3.2225e-02, -3.8450e-03],\n",
       "          [-6.8206e-02, -1.9445e-01, -1.4166e-01],\n",
       "          [-6.9528e-02, -1.8340e-01, -1.7422e-01]],\n",
       "\n",
       "         [[ 4.2781e-02, -6.7529e-02, -7.0309e-03],\n",
       "          [ 1.1765e-02, -1.4958e-01, -1.2361e-01],\n",
       "          [ 1.0205e-02, -1.0393e-01, -1.1742e-01]],\n",
       "\n",
       "         [[ 1.2661e-01,  8.5046e-02,  1.3066e-01],\n",
       "          [ 1.7585e-01,  1.1288e-01,  1.1937e-01],\n",
       "          [ 1.4656e-01,  9.8892e-02,  1.0348e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.2176e-02, -1.0766e-01, -2.6388e-01],\n",
       "          [ 2.7957e-01, -3.7416e-02, -2.5471e-01],\n",
       "          [ 3.4872e-01,  3.0041e-02, -5.5898e-02]],\n",
       "\n",
       "         [[ 2.5063e-01,  1.5543e-01, -1.7432e-01],\n",
       "          [ 3.9255e-01,  3.2306e-02, -3.5191e-01],\n",
       "          [ 1.9299e-01, -1.9898e-01, -2.9713e-01]],\n",
       "\n",
       "         [[ 4.6032e-01,  4.3399e-01,  2.8352e-01],\n",
       "          [ 1.6341e-01, -5.8165e-02, -1.9196e-01],\n",
       "          [-1.9521e-01, -4.5630e-01, -4.2732e-01]]]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_net.features[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = Image.open('./goldfish_1.jpg')\n",
    "img_2 = Image.open('./goose_99.jpg')\n",
    "img_3 = Image.open('./jellyfish_107.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_mean = (0.485, 0.456, 0.406)\n",
    "vgg_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "vgg_transform = transforms.Compose([transforms.Resize((256,256)), # H, W, C\n",
    "                                    transforms.ToTensor(), # C, H, W, rescale to [0, 1]\n",
    "                                    transforms.Normalize(mean=vgg_mean, std=vgg_std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_in = torch.zeros((3, 3, 256, 256), dtype=torch.float32) # B, C, H, W\n",
    "model_in[0] = vgg_transform(img_1)\n",
    "model_in[1] = vgg_transform(img_2)\n",
    "model_in[2] = vgg_transform(img_3)\n",
    "\n",
    "model_out = vgg16_net(model_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  99, 107])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Desired output: [1, 99, 107]\n",
    "model_out.detach().numpy().argmax(axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonics_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
